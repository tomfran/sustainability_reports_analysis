{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# CSV-link-processing\n",
    "\n",
    "## Introduzione\n",
    "\n",
    "L'obiettivo della repository è quello di filtrare i link pdf trovati dal crawler per identificare i bilanci di sostenibilità relativi all'anno 2018 pubblicati dalle aziende.\n",
    "\n",
    "## Processo\n",
    "\n",
    "Ogni riga del file csv presenta un sito web e una lista che contiene dizionari con le informazioni relative ad ogni link.\n",
    "\n",
    "Si controlla l'idoneità di ogni link per decidere se scartarlo o meno.\n",
    "\n",
    "Nel processo si tiene traccia di :\n",
    "\n",
    "1. totale dei siti web\n",
    "2. totale dei siti web che hanno pubblicato\n",
    "3. percentuale di questi siti sul totale\n",
    "4. totale file pdf\n",
    "5. numero di file pdf utili\n",
    "6. percentuale dei file utili sul totale\n",
    "7. profondità media alla quale si trovano questi file\n",
    "8. numero di aziende che pubblicano il bilancio in homepage \n",
    "\n",
    "Di seguito il programma principale, che utilizza le funzioni __evaluate__ e __get_depth__, descritte sotto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Useful links: \n",
      "{'www.iav.com': ['https://www.iav.com/app/uploads/2019/02/IAV_Sustainability_Report_2018.pdf']}\n",
      "\n",
      "Stats: \n",
      "Total websites: 7\n",
      "Websites who published: 1\n",
      "Published percentage: 14.285714\n",
      "Total links: 62\n",
      "Probable sustainability pdfs links: 1\n",
      "Useful pdfs percentage: 1.612903\n",
      "Average depth: 3.000000\n",
      "Pdfs in homepage: 0\n"
     ]
    }
   ],
   "source": [
    "import csv, re\n",
    "\n",
    "def clean_pdf_list(ll):\n",
    "    #keep only the link and the source (needed to get depth later)\n",
    "    ret = []\n",
    "    for l in ll:\n",
    "        k = {}\n",
    "        k['pdfUrl'] = l[0]['pdfUrl']\n",
    "        k['sourcePageUrl'] = l[0]['sourcePageUrl']\n",
    "        ret.append(k)\n",
    "\n",
    "    # remove #page= and GET arguments\n",
    "    for r in ret:\n",
    "        if \"#page=\" in r['pdfUrl']:\n",
    "            s = r['pdfUrl']\n",
    "            r['pdfUrl'] = s[:s.find(\"#page=\")]\n",
    "            s = r['pdfUrl']\n",
    "            i = s.find(\"?\") \n",
    "            j = s.find(\".pdf\")\n",
    "            if i > 0 and j < i:\n",
    "                r['pdfUrl'] = s[:s.find(\".pdf\")+4]\n",
    "    \n",
    "    #remove duplicates\n",
    "    s = set()\n",
    "    for d in ret:\n",
    "        s.add(d['pdfUrl'])\n",
    "    no_dup = []\n",
    "    for r in ret:\n",
    "        if r['pdfUrl'] in s:\n",
    "            no_dup.append(r)\n",
    "            s.remove(r['pdfUrl'])\n",
    "\n",
    "    return no_dup\n",
    "\n",
    "def get_stats(s):\n",
    "    return (\"Total websites,%d\\nWebsites who \\\n",
    "published,%d\\nPublished percentage,%f\\nTotal \\\n",
    "links,%d\\nProbable sustainability pdfs links,%d\\n\\\n",
    "Useful pdfs percentage,%f\\nAverage depth,%f\\n\\\n",
    "Pdfs in homepage,%d\" %(s[0], s[1], (s[1]/s[0])*100, s[2], s[3], (s[3]/s[2])*100, s[4], s[5]))\n",
    "\n",
    "\n",
    "csv_file = \"small.csv\"\n",
    "ret = {}\n",
    "csv.field_size_limit(100000000)\n",
    "with open(csv_file) as csv_file:    \n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    total = useful = depth_sum = homelinks = 0\n",
    "    counter = -1\n",
    "    pdf_dump = []\n",
    "    for row in csv_reader:\n",
    "        counter += 1            \n",
    "        if counter == 0 : continue\n",
    "        pdf_dump = eval(row[1])\n",
    "        total += len(pdf_dump)\n",
    "        pdf_dump = [k for k in [(l,evaluate(l)) for l in pdf_dump] if k[1][0]]\n",
    "        pdf_dump = clean_pdf_list(pdf_dump)\n",
    "        for l in pdf_dump:\n",
    "            d = get_depth(l)\n",
    "            if d == 1:\n",
    "                homelinks += 1\n",
    "            depth_sum += d\n",
    "\n",
    "        useful += len(pdf_dump)\n",
    "        if pdf_dump:\n",
    "            ret[row[0]] = [l['pdfUrl'] for l in pdf_dump] \n",
    "\n",
    "stats = (counter, len(ret), total, useful, depth_sum/useful, homelinks)\n",
    "\n",
    "print(\"Useful links: \\n%s\\n\" %ret)\n",
    "print(\"Stats: \\n%s\" %get_stats(stats).replace(\",\", \": \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get depth\n",
    "\n",
    "Funzione che resituisce la profondità alla quale si trova l'anchor del file pdf.\n",
    "\n",
    "Scarta eventuali livelli inutili, rappresentati ad esempio dalla lingua della pagina, un _/home_ , _/homepage_ , etc..\n",
    "\n",
    "Il _set di livelli_ da non contare è molto più grande, per semplicità è ridotto nell'esempio seguente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOT_COUNT_SET = set([\"it\", \"en\", \"de\", \"home\", \"homepage\"])\n",
    "\n",
    "def get_depth(l):\n",
    "    s = l[\"sourcePageUrl\"]\n",
    "    #\"remove https://\"\n",
    "    ss = s[s.find(\"://\")+3:]\n",
    "    #remove last / if present\n",
    "    if ss[-1] == \"/\":\n",
    "        ss = ss[:-1]\n",
    "    # count levels\n",
    "    d = ss.count(\"/\") +1\n",
    "    k = d\n",
    "    # remove unnecessary levels (eg: /home /en /it)\n",
    "    for i in range(d):\n",
    "        if ss[:ss.find(\"/\")] in NOT_COUNT_SET:\n",
    "            k -= 1\n",
    "        ss = ss[ss.find(\"/\")+1:]\n",
    "    return k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Funzione che si occupa di determinare se un file è ideoneo oppure no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_THRESHOLD = 40\n",
    "\n",
    "def evaluate(link):\n",
    "    score = 0\n",
    "    filename = link['pdfUrl'].split(\"/\")[-1].casefold()\n",
    "\n",
    "    url = link['pdfUrl'][link['pdfUrl'].find(\"://\")+3:]\n",
    "    url = url[url.find(\"/\")+1:-len(filename)]\n",
    "    anchor = link['anchor'].casefold()\n",
    "\n",
    "    if \"sostenibilit\" in filename or \"sostenibilit\" in anchor or \\\n",
    "       \"sustainability\" in filename or \"sustainability\" in anchor: \n",
    "        score += 40\n",
    "\n",
    "    if \"ambient\" in filename or \"ambient\" in anchor or \\\n",
    "       \"environment\" in filename or \"environment\" in anchor:\n",
    "        score += 20\n",
    "    \n",
    "    if \"bilancio\" in filename or \"bilancio\" in anchor or \\\n",
    "       \"balance\" in filename or \"balance\" in anchor:\n",
    "        score += 20\n",
    "\n",
    "    if \"rapporto\" in filename or \"rapporto\" in anchor or \\\n",
    "       \"report\" in filename or \"report\" in anchor:\n",
    "        score += 20\n",
    "\n",
    "    if \"sostenibilit\" in url or \"sustainability\" in url: \n",
    "        score += 10\n",
    "\n",
    "    if \"ambient\" in url or \"environment\" in url: \n",
    "        score += 10\n",
    "\n",
    "    year = int(\"2018\" in filename) + int(\"2018\" in url) + int(\"2018\" in anchor)\n",
    "\n",
    "    if re.match(r'(.*)20[0-2]([0-7]|[9])(.*)', filename) and not re.match(r'(.*)18(.*)', filename):\n",
    "        score = 0\n",
    "        \n",
    "    return score >= EVALUATION_THRESHOLD and year > 0, score, year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score\n",
    "\n",
    "Osservando i blocchi _if_ della funzione si può notare che:\n",
    "1. la presenza di __sostenibilità__ nel filename oppure nell'anchor incrementa lo score di molto, rendendola di fatto condizione sufficiente, se presente anche un'occorrenza di __2018__ , per la valutazione positiva di un pdf\n",
    "2. le parole __ambiente__ , __bilancio__ e __rapporto__ hanno meno importanza di __sostenibilità__ , aumentando lo score della metà\n",
    "3. la presenza di _sostenibilità_ o _ambiente_ nell'url contribuisce in maniera marginale rispetto al resto\n",
    "\n",
    "### Anno\n",
    "\n",
    "Dopo aver valutato lo score di un link, si passa alla ricerca di occorrenze di __2018__.\n",
    "Si conta quante volte compare nel filename, anchor e url.\n",
    "\n",
    "Infine si passa alla ricerca di file non idonei per quanto riguarda l'anno del bilancio di sostenibilità.\n",
    "\n",
    "Vi sono infatti casi in cui compaiono uno o più __2018__, ma il file non riguarda quell'anno, è il caso ad esempio di siti gestiti con Wordpress in cui nel 2018 è stato caricato un bilancio del 2017, in quel caso ci sarà un url del tipo /2018/mese/giorno/filename.\n",
    "\n",
    "Si vogliono però tenere i file che riguardano l'anno 2018 nonostante sia presente un 17, o un 19.\n",
    "È il caso di file come _bilancio sostenibilità 17-18.pdf_.\n",
    "\n",
    "### Valutazione\n",
    "\n",
    "La condizione necessaria per considerare un file idoneo oppure no, tiene in considerazione lo score e le occorrenze dell'anno su cui ci si concentra:\n",
    "Lo score deve essere almeno di 40, e deve essere essere presente almeno un occorrenza di 2018.\n",
    "\n",
    "È  stato scelto uno score di 40 per includere casi in cui è presente almeno _sostenibilità_ nel filename o nell'anchor, oppure una combinazione di _bilancio_ / _rapporto_ / _ambiente_.\n",
    "\n",
    "### Condizioni di valutazione alternative\n",
    "\n",
    "Alcune condizioni alternative sono le seguenti:\n",
    "\n",
    "```\n",
    "1. score >= EVALUATION_THRESHOLD and year > 0\n",
    "2. score >= EVALUATION_THRESHOLD and \"2018\" in filename\n",
    "3. score >= EVALUATION_THRESHOLD and \"2018\" in anchor\n",
    "4. score >= EVALUATION_THRESHOLD and (\"2018\" in filename or \"2018\" in anchor)\n",
    "5. score >= EVALUATION_THRESHOLD\n",
    "```\n",
    "\n",
    "Si può notare sempre lo stesso andamento per quanto riguarda le differenze tra le condizioni, mentre la threshold alza o abbassa l'andamento complessivo.\n",
    "\n",
    "## Test valutazioni\n",
    "\n",
    "A seguire i risultati della funzione con due valori di threshold, 40 e 60.\n",
    "L'ultima condizione non tiene conto che il pdf sia del 2018, si può notare che impostare qualsiasi tipo di restrizione sull'anno riduce di molto il numero di bilanci trovati.\n",
    "\n",
    "### Numero di siti web che hanno pubblicato il bilancio\n",
    "La stima iniziale di siti che hanno pubblicato, ovvero circa 4000, fatta prima di perferzionare la funzione __evaluation__ era evidentemente errata, infatti le parole chiave erano ricercate nell'url completo, compreso di dominio, senza rimuovere i duplicati e senza dare più importanza alla keyword _sostenibilità_.\n",
    "Il numero di riscontri ora è chiaramente minore, ma si apprezza una pertinenza con il tema della sostenibilità molto più elevata.\n",
    "<img src=\"graphs/0.png\" style=\"width: 400px;\">\n",
    "\n",
    "### Percentuale nel totale\n",
    "<img src=\"graphs/1.png\" style=\"width: 400px;\">\n",
    "\n",
    "### Numero di pdf utili\n",
    "Si nota che il numero di pdf è sempre maggiore del numero di siti web che pubblicano, questo perchè circa il __18%__ dei siti pubblica un numero che varia da 2 a 4 di pdf che sono ritenuti utili.\n",
    "Può succedere perchè in alcuni casi sono traduzioni dello stesso file, o in alcuni casi un summary, accompagnato dal file completo.\n",
    "Ritengo che si possa accettare uno scenario del genere per il momento, per poi gestirlo nella successiva parte di analisi semantica del testo, andando ad \"unire\" le entità trovate dai vari file.\n",
    "<img src=\"graphs/2.png\" style=\"width: 400px;\">\n",
    "\n",
    "### Percentuale di file pdf utili\n",
    "<img src=\"graphs/3.png\" style=\"width: 400px;\">\n",
    "\n",
    "### Profondità media dei file\n",
    "<img src=\"graphs/4.png\" style=\"width: 400px;\">\n",
    "\n",
    "### Link presenti in homepage\n",
    "<img src=\"graphs/5.png\" style=\"width: 400px;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
